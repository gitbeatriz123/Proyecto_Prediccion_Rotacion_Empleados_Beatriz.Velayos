{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44855fb6-9ea2-4a31-8707-f4cf9a8a0af0",
   "metadata": {},
   "source": [
    "1) Setup y carga del Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef0a89e-8dde-46a0-9b23-7269d1641902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filas: 1470 | Attrition rate: 0.1612\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[*]\").appName(\"kpi-dashboard\").getOrCreate())\n",
    "df = spark.read.parquet(\"/data/processed/employee_attrition.parquet\")\n",
    "\n",
    "TOTAL = df.count()\n",
    "POS   = df.filter(F.col(\"attrition_label\")==1).count()\n",
    "RATE  = POS / TOTAL if TOTAL else 0.0\n",
    "\n",
    "print(\"Total filas:\", TOTAL, \"| Attrition rate:\", round(RATE,4))\n",
    "os.makedirs(\"/output/bi\", exist_ok=True)\n",
    "os.makedirs(\"/output/plots\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b0499-440d-412f-8800-90db947f2a0a",
   "metadata": {},
   "source": [
    "2) KPI general (overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f7ec951-2026-4ded-aca8-f10185118215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>attrition_positive</th>\n",
       "      <th>attrition_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1470</td>\n",
       "      <td>237</td>\n",
       "      <td>0.161224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rows  attrition_positive  attrition_rate\n",
       "0  1470                 237        0.161224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "kpi_overview = pd.DataFrame([{\"rows\": TOTAL, \"attrition_positive\": POS, \"attrition_rate\": RATE}])\n",
    "kpi_overview.to_csv(\"/output/bi/kpi_overview.csv\", index=False)\n",
    "kpi_overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a46f16-7089-4965-8aff-200a3c373522",
   "metadata": {},
   "source": [
    "3) Tasa por JobRole / Department / OverTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13233da5-a213-47af-8776-8cc0432d5660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|             JobRole|  n|      attrition_rate|\n",
      "+--------------------+---+--------------------+\n",
      "|Sales Representative| 83| 0.39759036144578314|\n",
      "|Laboratory Techni...|259| 0.23938223938223938|\n",
      "|     Human Resources| 52| 0.23076923076923078|\n",
      "|     Sales Executive|326| 0.17484662576687116|\n",
      "|  Research Scientist|292| 0.16095890410958905|\n",
      "|Manufacturing Dir...|145| 0.06896551724137931|\n",
      "|Healthcare Repres...|131| 0.06870229007633588|\n",
      "|             Manager|102|0.049019607843137254|\n",
      "|   Research Director| 80|               0.025|\n",
      "+--------------------+---+--------------------+\n",
      "\n",
      "+--------------------+---+-------------------+\n",
      "|          Department|  n|     attrition_rate|\n",
      "+--------------------+---+-------------------+\n",
      "|               Sales|446| 0.2062780269058296|\n",
      "|     Human Resources| 63|0.19047619047619047|\n",
      "|Research & Develo...|961| 0.1383975026014568|\n",
      "+--------------------+---+-------------------+\n",
      "\n",
      "+--------+----+-------------------+\n",
      "|OverTime|   n|     attrition_rate|\n",
      "+--------+----+-------------------+\n",
      "|     Yes| 416|0.30528846153846156|\n",
      "|      No|1054|0.10436432637571158|\n",
      "+--------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def agg_rate(df, col):\n",
    "    return (df.groupBy(col)\n",
    "              .agg(F.count(\"*\").alias(\"n\"),\n",
    "                   F.avg(\"attrition_label\").alias(\"attrition_rate\"))\n",
    "              .orderBy(F.desc(\"attrition_rate\")))\n",
    "\n",
    "jobrole  = agg_rate(df, \"JobRole\")\n",
    "depart   = agg_rate(df, \"Department\")\n",
    "overtime = agg_rate(df, \"OverTime\")\n",
    "\n",
    "jobrole.toPandas().to_csv(\"/output/bi/attrition_by_jobrole.csv\", index=False)\n",
    "depart.toPandas().to_csv(\"/output/bi/attrition_by_department.csv\", index=False)\n",
    "overtime.toPandas().to_csv(\"/output/bi/attrition_by_overtime.csv\", index=False)\n",
    "\n",
    "jobrole.show(10); depart.show(10); overtime.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffef056-7eaa-467d-97c6-e0c5fcf54e27",
   "metadata": {},
   "source": [
    "4) Tasa por tramos de salario y de antigüedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d59c3e61-40f3-45ae-ad5c-8c4f2cae4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+\n",
      "|income_band|  n|     attrition_rate|\n",
      "+-----------+---+-------------------+\n",
      "|     40–60k|307|0.14332247557003258|\n",
      "|     60–80k|249|0.08835341365461848|\n",
      "|    80–100k|102|0.13725490196078433|\n",
      "|       <40k|442| 0.2692307692307692|\n",
      "|     >=100k|370|0.10270270270270271|\n",
      "+-----------+---+-------------------+\n",
      "\n",
      "+-----------+---+-------------------+\n",
      "|tenure_band|  n|     attrition_rate|\n",
      "+-----------+---+-------------------+\n",
      "|        0–2|215| 0.3488372093023256|\n",
      "|        2–5|365|0.18082191780821918|\n",
      "|       5–10|524|0.11068702290076336|\n",
      "|       >=10|366|0.10382513661202186|\n",
      "+-----------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salario anual ya lo creaste como 'income_yearly' en el ETL\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_bins = (df\n",
    "  .withColumn(\"income_band\",\n",
    "              F.when(F.col(\"income_yearly\") < 40000, \"<40k\")\n",
    "               .when(F.col(\"income_yearly\") < 60000, \"40–60k\")\n",
    "               .when(F.col(\"income_yearly\") < 80000, \"60–80k\")\n",
    "               .when(F.col(\"income_yearly\") < 100000,\"80–100k\")\n",
    "               .otherwise(\">=100k\"))\n",
    "  .withColumn(\"tenure_band\",\n",
    "              F.when(F.col(\"YearsAtCompany\") < 2, \"0–2\")\n",
    "               .when(F.col(\"YearsAtCompany\") < 5, \"2–5\")\n",
    "               .when(F.col(\"YearsAtCompany\") < 10,\"5–10\")\n",
    "               .otherwise(\">=10\"))\n",
    ")\n",
    "\n",
    "income_rate = (df_bins.groupBy(\"income_band\")\n",
    "                .agg(F.count(\"*\").alias(\"n\"), F.avg(\"attrition_label\").alias(\"attrition_rate\"))\n",
    "                .orderBy(\"income_band\"))\n",
    "tenure_rate = (df_bins.groupBy(\"tenure_band\")\n",
    "                .agg(F.count(\"*\").alias(\"n\"), F.avg(\"attrition_label\").alias(\"attrition_rate\"))\n",
    "                .orderBy(\"tenure_band\"))\n",
    "\n",
    "income_rate.toPandas().to_csv(\"/output/bi/attrition_by_income_band.csv\", index=False)\n",
    "tenure_rate.toPandas().to_csv(\"/output/bi/attrition_by_tenure_band.csv\", index=False)\n",
    "\n",
    "income_rate.show(); tenure_rate.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1559d2-fe18-4a1e-80ca-58479d819167",
   "metadata": {},
   "source": [
    "5) Exportar coeficientes (LogReg) o importancias (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a91d6996-7d4d-4dd7-8c5c-3485f9988bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes (LR) -> /output/bi/feature_effects.csv\n"
     ]
    }
   ],
   "source": [
    "import json, pandas as pd, joblib, os\n",
    "\n",
    "# Carga robusta (soporta .pkl como dict {\"model\":..., \"threshold\":...} o como Pipeline directamente)\n",
    "bundle = joblib.load(\"/output/models/logreg_baseline.pkl\")\n",
    "\n",
    "if isinstance(bundle, dict) and \"model\" in bundle:\n",
    "    mdl = bundle[\"model\"]\n",
    "    thr = bundle.get(\"threshold\", 0.5)\n",
    "else:\n",
    "    mdl = bundle         # el .pkl es directamente el Pipeline\n",
    "    thr = 0.5            # si no guardaste threshold, usa 0.5 por defecto\n",
    "\n",
    "# 'pre' es el ColumnTransformer dentro del Pipeline\n",
    "pre = mdl.named_steps[\"prep\"]\n",
    "\n",
    "# Reconstruir nombres de features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def ohe_feature_names(preproc, cat_cols):\n",
    "    ohe = preproc.named_transformers_[\"cat\"]\n",
    "    return ohe.get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "# Obtener las columnas originales desde el dataset\n",
    "import numpy as np\n",
    "pdf = df.toPandas()\n",
    "drop_cols = [\"attrition_label\",\"Attrition\",\"EmployeeNumber\",\"EmployeeCount\",\"StandardHours\",\"Over18\"]\n",
    "X = pdf.drop(columns=[c for c in drop_cols if c in pdf.columns], errors=\"ignore\")\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "feat_names = num_cols + ohe_feature_names(pre, cat_cols)\n",
    "\n",
    "# Exportar coeficientes (LR) o importancias (RF)\n",
    "out_path = \"/output/bi/feature_effects.csv\"\n",
    "os.makedirs(\"/output/bi\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    coefs = mdl.named_steps[\"clf\"].coef_.ravel()\n",
    "    pd.DataFrame({\"feature\": feat_names, \"effect\": coefs}) \\\n",
    "      .sort_values(\"effect\", ascending=False) \\\n",
    "      .to_csv(out_path, index=False)\n",
    "    print(\"Coeficientes (LR) ->\", out_path)\n",
    "except Exception:\n",
    "    importances = mdl.named_steps[\"clf\"].feature_importances_\n",
    "    pd.DataFrame({\"feature\": feat_names, \"importance\": importances}) \\\n",
    "      .sort_values(\"importance\", ascending=False) \\\n",
    "      .to_csv(out_path, index=False)\n",
    "    print(\"Importancias (RF) ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43497551-e79f-4329-a597-ab5f1fe6e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escrito -> public.feature_effects\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "\n",
    "# 1) Lee el CSV que acabas de generar\n",
    "df = pd.read_csv(\"/output/bi/feature_effects.csv\")\n",
    "\n",
    "# Normaliza nombre de columna a 'value' para que el visual sea único\n",
    "if \"effect\" in df.columns:\n",
    "    df = df.rename(columns={\"effect\":\"value\"})\n",
    "elif \"importance\" in df.columns:\n",
    "    df = df.rename(columns={\"importance\":\"value\"})\n",
    "\n",
    "# etiqueta del modelo (ajusta si estás mostrando RF)\n",
    "df[\"model_name\"] = \"LogReg\"\n",
    "\n",
    "# 2) Escribe en Postgres\n",
    "eng = sa.create_engine(\"postgresql+psycopg2://ml:ml@postgres:5432/mlops\")\n",
    "df.to_sql(\"feature_effects\", eng, schema=\"public\", if_exists=\"replace\", index=False)\n",
    "print(\"Escrito -> public.feature_effects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a71ded-66cc-44af-964d-6f4a29b8d305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
